{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa055581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910bf128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b92528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the doc text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Tag each token\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Converts each token to lowercase and lemmatizes it (according to its POS tag)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Perform lemmatization; call get_wordnet_pos function: change to corresponding WordNet POS\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower(), pos=get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "    \n",
    "    # Filtering: Get rid of stopwords and non-alphabetic token\n",
    "    stop_words = set(stopwords.words('english')) # Load stopwords\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e613563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text processing\n",
    "df['processed_description'] = df['description'].fillna('').apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fd3e986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  Food52, a fast-growing, James Beard Award-winn...   \n",
      "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
      "2  Our client, located in Houston, is actively se...   \n",
      "3  THE COMPANY: ESRI â€“ Environmental Systems Rese...   \n",
      "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
      "\n",
      "                               processed_description  \n",
      "0  james beard online food community curated reci...  \n",
      "1  organise focus vibrant awesome passion custome...  \n",
      "2  client locate houston actively seek experience...  \n",
      "3  company esri environmental system research ins...  \n",
      "4  job title itemization review managerlocation f...  \n"
     ]
    }
   ],
   "source": [
    "print(df[['description', 'processed_description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40aac674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bab046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF analysis\n",
    "\n",
    "# Load raal and fake job description\n",
    "real_descriptions = df[df['fraudulent'] == 0]['processed_description'].tolist()\n",
    "fake_descriptions = df[df['fraudulent'] == 1]['processed_description'].tolist()\n",
    "\n",
    "descriptions = real_descriptions + fake_descriptions\n",
    "labels = [0] * len(real_descriptions) + [1] * len(fake_descriptions)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names) # Feature names as column names\n",
    "tfidf_df['fraudulent'] = labels  # Add label as a new column\n",
    "\n",
    "real_tfidf_mean = tfidf_df[tfidf_df['fraudulent'] == 0].mean(axis=0).sort_values(ascending=False)\n",
    "fake_tfidf_mean = tfidf_df[tfidf_df['fraudulent'] == 1].mean(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce14c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in real postings:\n",
      "work          0.035460\n",
      "team          0.035180\n",
      "customer      0.033374\n",
      "sale          0.027176\n",
      "product       0.027086\n",
      "client        0.026883\n",
      "service       0.026470\n",
      "business      0.025076\n",
      "experience    0.024584\n",
      "company       0.024304\n",
      "dtype: float64\n",
      "\n",
      "Top words in fake postings:\n",
      "fraudulent    1.000000\n",
      "work          0.048301\n",
      "position      0.037100\n",
      "customer      0.035093\n",
      "amp           0.033624\n",
      "service       0.033064\n",
      "home          0.031776\n",
      "skill         0.027830\n",
      "time          0.026968\n",
      "project       0.025612\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Top words in real postings:\")\n",
    "print(real_tfidf_mean.head(10))\n",
    "\n",
    "print(\"\\nTop words in fake postings:\")\n",
    "print(fake_tfidf_mean.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a4c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis for title:\n",
      "Real job postings - missing values: 0\n",
      "Fake job postings - missing values: 0\n",
      "Average length of title in real jobs: 28.42159398142706\n",
      "Average length of title in fake jobs: 30.666281755196305\n",
      "\n",
      "Analysis for company_profile:\n",
      "Real job postings - missing values: 2721\n",
      "Fake job postings - missing values: 587\n",
      "Average length of company_profile in real jobs: 762.734625341076\n",
      "Average length of company_profile in fake jobs: 716.673835125448\n",
      "\n",
      "Analysis for location:\n",
      "Real job postings - missing values: 327\n",
      "Fake job postings - missing values: 19\n",
      "Average length of location in real jobs: 15.636843051477198\n",
      "Average length of location in fake jobs: 15.206611570247935\n",
      "\n",
      "Analysis for requirements:\n",
      "Real job postings - missing values: 2541\n",
      "Fake job postings - missing values: 154\n",
      "Average length of requirements in real jobs: 702.3622607614178\n",
      "Average length of requirements in fake jobs: 542.5266853932584\n"
     ]
    }
   ],
   "source": [
    "# Fields analysis (e.g. titile, company profile, ...)\n",
    "\n",
    "def compare_field_statistics(df, field_name):\n",
    "    real_jobs = df[df['fraudulent'] == 0][field_name].dropna()\n",
    "    fake_jobs = df[df['fraudulent'] == 1][field_name].dropna()\n",
    "\n",
    "    print(f\"\\nAnalysis for {field_name}:\")\n",
    "    print(f\"Real job postings - missing values: {df[df['fraudulent'] == 0][field_name].isnull().sum()}\")\n",
    "    print(f\"Fake job postings - missing values: {df[df['fraudulent'] == 1][field_name].isnull().sum()}\")\n",
    "    \n",
    "    print(f\"Average length of {field_name} in real jobs: {real_jobs.apply(len).mean()}\")\n",
    "    print(f\"Average length of {field_name} in fake jobs: {fake_jobs.apply(len).mean()}\")\n",
    "\n",
    "compare_field_statistics(df, 'title')\n",
    "compare_field_statistics(df, 'company_profile')\n",
    "compare_field_statistics(df, 'location')\n",
    "compare_field_statistics(df, 'requirements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7836f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 bigrams in real job postings:\n",
      "                      ngram  count\n",
      "104134     customer service   2594\n",
      "404848        social medium   1422\n",
      "434273          team member   1168\n",
      "230152            join team   1130\n",
      "476317         work closely   1085\n",
      "203036      ideal candidate   1067\n",
      "197426           high level   1048\n",
      "77197   communication skill    864\n",
      "481530      year experience    793\n",
      "122462     development team    770\n",
      "\n",
      "Top 10 bigrams in fake job postings:\n",
      "                     ngram  count\n",
      "7071      customer service    166\n",
      "20180              oil gas    144\n",
      "7232            data entry    121\n",
      "12586         gas industry    117\n",
      "33623            work home    109\n",
      "1128         aker solution    104\n",
      "5149   communication skill     90\n",
      "23728      product service     83\n",
      "3590       business people     74\n",
      "145              able work     72\n",
      "\n",
      "Top 10 trigrams in real job postings:\n",
      "                              ngram  count\n",
      "551140                 play kid pay    715\n",
      "797975             usd monthly cost    687\n",
      "158888          cost living housing    677\n",
      "478630          monthly cost living    677\n",
      "47077              asia usd monthly    658\n",
      "531995              pay love travel    623\n",
      "397910                 kid pay love    623\n",
      "731160  student cardsgabriel adkins    615\n",
      "430539              love travel job    598\n",
      "786345              travel job asia    592\n",
      "\n",
      "Top 10 trigrams in fake job postings:\n",
      "                              ngram  count\n",
      "23943              oil gas industry    113\n",
      "2271   approximately people country     54\n",
      "25464    people country information     54\n",
      "28272  production maximize recovery     54\n",
      "13972    field employ approximately     54\n",
      "28178           product service oil     54\n",
      "3841     bring discovery production     54\n",
      "25993        petroleum field employ     54\n",
      "17515   information business people     54\n",
      "17413   industry engineering design     54\n"
     ]
    }
   ],
   "source": [
    "# N-gram analysis: find common word pairs\n",
    "def ngram_analysis(descriptions, n=2):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    # Row: job description; column: specific n-gram phrase; element: number of occurrences of an n-gram in the corresponding job description\n",
    "    ngram_matrix = vectorizer.fit_transform(descriptions)\n",
    "    \n",
    "    # Sum the columns: get the total number of occurrences of each n-gram\n",
    "    ngram_counts = ngram_matrix.sum(axis=0).A1\n",
    "    ngram_features = vectorizer.get_feature_names()\n",
    "    \n",
    "    ngrams = pd.DataFrame({'ngram': ngram_features, 'count': ngram_counts})\n",
    "    return ngrams.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "# show top 10 bigrams and trigrams\n",
    "print(\"Top 10 bigrams in real job postings:\")\n",
    "real_bigrams = ngram_analysis(real_descriptions, n=2)\n",
    "print(real_bigrams)\n",
    "\n",
    "print(\"\\nTop 10 bigrams in fake job postings:\")\n",
    "fake_bigrams = ngram_analysis(fake_descriptions, n=2)\n",
    "print(fake_bigrams)\n",
    "\n",
    "print(\"\\nTop 10 trigrams in real job postings:\")\n",
    "real_trigrams = ngram_analysis(real_descriptions, n=3)\n",
    "print(real_trigrams)\n",
    "\n",
    "print(\"\\nTop 10 trigrams in fake job postings:\")\n",
    "fake_trigrams = ngram_analysis(fake_descriptions, n=3)\n",
    "print(fake_trigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
